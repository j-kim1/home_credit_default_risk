{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broad-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-australia",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "related-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    prev_dtype = {\n",
    "        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'HOUR_APPR_PROCESS_START':np.int32, 'NFLAG_LAST_APPL_IN_DAY':np.int32,\n",
    "        'DAYS_DECISION':np.int32, 'SELLERPLACE_AREA':np.int32, 'AMT_ANNUITY':np.float32, 'AMT_APPLICATION':np.float32,\n",
    "        'AMT_CREDIT':np.float32, 'AMT_DOWN_PAYMENT':np.float32, 'AMT_GOODS_PRICE':np.float32, 'RATE_DOWN_PAYMENT':np.float32,\n",
    "        'RATE_INTEREST_PRIMARY':np.float32, 'RATE_INTEREST_PRIVILEGED':np.float32, 'CNT_PAYMENT':np.float32,\n",
    "        'DAYS_FIRST_DRAWING':np.float32, 'DAYS_FIRST_DUE':np.float32, 'DAYS_LAST_DUE_1ST_VERSION':np.float32,\n",
    "        'DAYS_LAST_DUE':np.float32, 'DAYS_TERMINATION':np.float32, 'NFLAG_INSURED_ON_APPROVAL':np.float32\n",
    "    }\n",
    "    \n",
    "    bureau_dtype = {\n",
    "        'SK_ID_CURR':np.uint32, 'SK_ID_BUREAU':np.uint32, 'DAYS_CREDIT':np.int32,'CREDIT_DAY_OVERDUE':np.int32,\n",
    "        'CNT_CREDIT_PROLONG':np.int32, 'DAYS_CREDIT_UPDATE':np.int32, 'DAYS_CREDIT_ENDDATE':np.float32,\n",
    "        'DAYS_ENDDATE_FACT':np.float32, 'AMT_CREDIT_MAX_OVERDUE':np.float32, 'AMT_CREDIT_SUM':np.float32,\n",
    "        'AMT_CREDIT_SUM_DEBT':np.float32, 'AMT_CREDIT_SUM_LIMIT':np.float32, 'AMT_CREDIT_SUM_OVERDUE':np.float32,\n",
    "        'AMT_ANNUITY':np.float32\n",
    "    }\n",
    "    \n",
    "    bureau_bal_dtype = {\n",
    "        'SK_ID_BUREAU':np.int32, 'MONTHS_BALANCE':np.int32,\n",
    "    }\n",
    "    \n",
    "    pos_dtype = {\n",
    "        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'MONTHS_BALANCE':np.int32, 'SK_DPD':np.int32,\n",
    "        'SK_DPD_DEF':np.int32, 'CNT_INSTALMENT':np.float32,'CNT_INSTALMENT_FUTURE':np.float32\n",
    "    }\n",
    "    \n",
    "    install_dtype = {\n",
    "        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'NUM_INSTALMENT_NUMBER':np.int32, 'NUM_INSTALMENT_VERSION':np.float32,\n",
    "        'DAYS_INSTALMENT':np.float32, 'DAYS_ENTRY_PAYMENT':np.float32, 'AMT_INSTALMENT':np.float32, 'AMT_PAYMENT':np.float32\n",
    "    }\n",
    "    \n",
    "    card_dtype = {\n",
    "        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'MONTHS_BALANCE':np.int16,\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL':np.int32, 'CNT_DRAWINGS_CURRENT':np.int32, 'SK_DPD':np.int32,'SK_DPD_DEF':np.int32,\n",
    "        'AMT_BALANCE':np.float32, 'AMT_DRAWINGS_ATM_CURRENT':np.float32, 'AMT_DRAWINGS_CURRENT':np.float32,\n",
    "        'AMT_DRAWINGS_OTHER_CURRENT':np.float32, 'AMT_DRAWINGS_POS_CURRENT':np.float32, 'AMT_INST_MIN_REGULARITY':np.float32,\n",
    "        'AMT_PAYMENT_CURRENT':np.float32, 'AMT_PAYMENT_TOTAL_CURRENT':np.float32, 'AMT_RECEIVABLE_PRINCIPAL':np.float32,\n",
    "        'AMT_RECIVABLE':np.float32, 'AMT_TOTAL_RECEIVABLE':np.float32, 'CNT_DRAWINGS_ATM_CURRENT':np.float32,\n",
    "        'CNT_DRAWINGS_OTHER_CURRENT':np.float32, 'CNT_DRAWINGS_POS_CURRENT':np.float32, 'CNT_INSTALMENT_MATURE_CUM':np.float32\n",
    "    }\n",
    "    data_path = 'D:\\workspace1\\Kaggle_Advanced_ML\\data'\n",
    "    app_train = pd.read_csv(os.path.join(data_path, 'application_train.csv'))\n",
    "    app_test = pd.read_csv(os.path.join(data_path,'application_test.csv'))\n",
    "    apps = pd.concat([app_train, app_test])\n",
    "    prev = pd.read_csv(os.path.join(data_path,'previous_application.csv'), dtype=prev_dtype)\n",
    "    bureau = pd.read_csv(os.path.join(data_path,'bureau.csv'), dtype=bureau_dtype)\n",
    "    bureau_bal = pd.read_csv(os.path.join(data_path,'bureau_balance.csv'), dtype=bureau_bal_dtype)\n",
    "    pos_bal = pd.read_csv(os.path.join(data_path,'POS_CASH_balance.csv'), dtype=pos_dtype)\n",
    "    install = pd.read_csv(os.path.join(data_path,'installments_payments.csv'), dtype=install_dtype)\n",
    "    card_bal = pd.read_csv(os.path.join(data_path,'credit_card_balance.csv'), dtype=card_dtype)\n",
    "\n",
    "    return apps, prev, bureau, bureau_bal, pos_bal, install, card_bal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-australian",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "second-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def get_apps_processed(apps):\n",
    "    \n",
    "    # EXT_SOURCE_X FEATURE 가공\n",
    "    apps['APPS_EXT_SOURCE_MEAN'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    apps['APPS_EXT_SOURCE_STD'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    apps['APPS_EXT_SOURCE_STD'] = apps['APPS_EXT_SOURCE_STD'].fillna(apps['APPS_EXT_SOURCE_STD'].mean())\n",
    "    \n",
    "    # AMT_CREDIT 비율로 Feature 가공\n",
    "    apps['APPS_ANNUITY_CREDIT_RATIO'] = apps['AMT_ANNUITY']/apps['AMT_CREDIT']\n",
    "    apps['APPS_GOODS_CREDIT_RATIO'] = apps['AMT_GOODS_PRICE']/apps['AMT_CREDIT']\n",
    "    \n",
    "    # AMT_INCOME_TOTAL 비율로 Feature 가공\n",
    "    apps['APPS_ANNUITY_INCOME_RATIO'] = apps['AMT_ANNUITY']/apps['AMT_INCOME_TOTAL']\n",
    "    apps['APPS_CREDIT_INCOME_RATIO'] = apps['AMT_CREDIT']/apps['AMT_INCOME_TOTAL']\n",
    "    apps['APPS_GOODS_INCOME_RATIO'] = apps['AMT_GOODS_PRICE']/apps['AMT_INCOME_TOTAL']\n",
    "    apps['APPS_CNT_FAM_INCOME_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['CNT_FAM_MEMBERS']\n",
    "    \n",
    "    # DAYS_BIRTH, DAYS_EMPLOYED 비율로 Feature 가공\n",
    "    apps['APPS_EMPLOYED_BIRTH_RATIO'] = apps['DAYS_EMPLOYED']/apps['DAYS_BIRTH']\n",
    "    apps['APPS_INCOME_EMPLOYED_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['DAYS_EMPLOYED']\n",
    "    apps['APPS_INCOME_BIRTH_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['DAYS_BIRTH']\n",
    "    apps['APPS_CAR_BIRTH_RATIO'] = apps['OWN_CAR_AGE'] / apps['DAYS_BIRTH']\n",
    "    apps['APPS_CAR_EMPLOYED_RATIO'] = apps['OWN_CAR_AGE'] / apps['DAYS_EMPLOYED']\n",
    "    \n",
    "    return apps\n",
    "\n",
    "def get_prev_processed(prev):\n",
    "    # 대출 신청 금액과 실제 대출액/대출 상품금액 차이 및 비율\n",
    "    prev['PREV_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n",
    "    prev['PREV_CREDIT_APPL_RATIO'] = prev['AMT_CREDIT']/prev['AMT_APPLICATION']\n",
    "    # prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']\n",
    "    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE']/prev['AMT_APPLICATION']\n",
    "    \n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # 첫번째 만기일과 마지막 만기일까지의 기간\n",
    "    prev['PREV_DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
    "    # 매월 납부 금액과 납부 횟수 곱해서 전체 납부 금액 구함. \n",
    "    all_pay = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
    "    # 전체 납부 금액 대비 AMT_CREDIT 비율을 구하고 여기에 다시 납부횟수로 나누어서 이자율 계산. \n",
    "    prev['PREV_INTERESTS_RATE'] = (all_pay/prev['AMT_CREDIT'] - 1)/prev['CNT_PAYMENT']\n",
    "        \n",
    "    return prev\n",
    "    \n",
    "    \n",
    "def get_prev_amt_agg(prev):\n",
    "    # 새롭게 생성된 대출 신청액 대비 다른 금액 차이 및 비율로 aggregation 수행. \n",
    "    agg_dict = {\n",
    "         # 기존 컬럼 aggregation. \n",
    "        'SK_ID_CURR':['count'],\n",
    "        'AMT_CREDIT':['mean', 'max', 'sum'],\n",
    "        'AMT_ANNUITY':['mean', 'max', 'sum'], \n",
    "        'AMT_APPLICATION':['mean', 'max', 'sum'],\n",
    "        'AMT_DOWN_PAYMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_GOODS_PRICE':['mean', 'max', 'sum'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        # 가공 컬럼 aggregation\n",
    "        'PREV_CREDIT_DIFF':['mean', 'max', 'sum'], \n",
    "        'PREV_CREDIT_APPL_RATIO':['mean', 'max'],\n",
    "        'PREV_GOODS_DIFF':['mean', 'max', 'sum'],\n",
    "        'PREV_GOODS_APPL_RATIO':['mean', 'max'],\n",
    "        'PREV_DAYS_LAST_DUE_DIFF':['mean', 'max', 'sum'],\n",
    "        'PREV_INTERESTS_RATE':['mean', 'max']\n",
    "    }\n",
    "\n",
    "    prev_group = prev.groupby('SK_ID_CURR')\n",
    "    prev_amt_agg = prev_group.agg(agg_dict)\n",
    "\n",
    "    # multi index 컬럼을 '_'로 연결하여 컬럼명 변경\n",
    "    prev_amt_agg.columns = [\"PREV_\"+ \"_\".join(x).upper() for x in prev_amt_agg.columns.ravel()]\n",
    "    \n",
    "    return prev_amt_agg\n",
    "\n",
    "def get_prev_refused_appr_agg(prev):\n",
    "    # 원래 groupby 컬럼 + 세부 기준 컬럼으로 groupby 수행. 세분화된 레벨로 aggregation 수행 한 뒤에 unstack()으로 컬럼레벨로 변형. \n",
    "    prev_refused_appr_group = prev[prev['NAME_CONTRACT_STATUS'].isin(['Approved', 'Refused'])].groupby([ 'SK_ID_CURR', 'NAME_CONTRACT_STATUS'])\n",
    "    prev_refused_appr_agg = prev_refused_appr_group['SK_ID_CURR'].count().unstack()\n",
    "    # 컬럼명 변경. \n",
    "    prev_refused_appr_agg.columns = ['PREV_APPROVED_COUNT', 'PREV_REFUSED_COUNT' ]\n",
    "    # NaN값은 모두 0으로 변경. \n",
    "    prev_refused_appr_agg = prev_refused_appr_agg.fillna(0)\n",
    "    \n",
    "    return prev_refused_appr_agg\n",
    "\n",
    "#### DAYS_DECISION이 -365일 이전 데이터 세트 추가 가공. \n",
    "def get_prev_days365_agg(prev):\n",
    "    cond_days365 = prev['DAYS_DECISION'] > -365\n",
    "    prev_days365_group = prev[cond_days365].groupby('SK_ID_CURR')\n",
    "    agg_dict = {\n",
    "         # 기존 컬럼 aggregation. \n",
    "        'SK_ID_CURR':['count'],\n",
    "        'AMT_CREDIT':['mean', 'max', 'sum'],\n",
    "        'AMT_ANNUITY':['mean', 'max', 'sum'], \n",
    "        'AMT_APPLICATION':['mean', 'max', 'sum'],\n",
    "        'AMT_DOWN_PAYMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_GOODS_PRICE':['mean', 'max', 'sum'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        # 가공 컬럼 aggregation\n",
    "        'PREV_CREDIT_DIFF':['mean', 'max', 'sum'], \n",
    "        'PREV_CREDIT_APPL_RATIO':['mean', 'max'],\n",
    "        'PREV_GOODS_DIFF':['mean', 'max', 'sum'],\n",
    "        'PREV_GOODS_APPL_RATIO':['mean', 'max'],\n",
    "        'PREV_DAYS_LAST_DUE_DIFF':['mean', 'max', 'sum'],\n",
    "        'PREV_INTERESTS_RATE':['mean', 'max']\n",
    "    }\n",
    "    \n",
    "    prev_days365_agg = prev_days365_group.agg(agg_dict)\n",
    "\n",
    "    # multi index 컬럼을 '_'로 연결하여 컬럼명 변경\n",
    "    prev_days365_agg.columns = [\"PREV_D365_\"+ \"_\".join(x).upper() for x in prev_days365_agg.columns.ravel()]\n",
    "    \n",
    "    return prev_days365_agg\n",
    "    \n",
    "def get_prev_agg(prev):\n",
    "    prev = get_prev_processed(prev)\n",
    "    prev_amt_agg = get_prev_amt_agg(prev)\n",
    "    prev_refused_appr_agg = get_prev_refused_appr_agg(prev)\n",
    "    prev_days365_agg = get_prev_days365_agg(prev)\n",
    "    \n",
    "    # prev_amt_agg와 조인. \n",
    "    prev_agg = prev_amt_agg.merge(prev_refused_appr_agg, on='SK_ID_CURR', how='left')\n",
    "    prev_agg = prev_agg.merge(prev_days365_agg, on='SK_ID_CURR', how='left')\n",
    "    # SK_ID_CURR별 과거 대출건수 대비 APPROVED_COUNT 및 REFUSED_COUNT 비율 생성. \n",
    "    prev_agg['PREV_REFUSED_RATIO'] = prev_agg['PREV_REFUSED_COUNT']/prev_agg['PREV_SK_ID_CURR_COUNT']\n",
    "    prev_agg['PREV_APPROVED_RATIO'] = prev_agg['PREV_APPROVED_COUNT']/prev_agg['PREV_SK_ID_CURR_COUNT']\n",
    "    # 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT' 컬럼 drop \n",
    "    prev_agg = prev_agg.drop(['PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT'], axis=1)\n",
    "    \n",
    "    return prev_agg\n",
    "\n",
    "# bureau 채무 완료 날짜 및 대출 금액 대비 채무 금액 관련 컬럼 가공.\n",
    "def get_bureau_processed(bureau):\n",
    "    # 예정 채무 시작 및 완료일과 실제 채무 완료일간의 차이 및 날짜 비율 가공.  \n",
    "    bureau['BUREAU_ENDDATE_FACT_DIFF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    bureau['BUREAU_CREDIT_FACT_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    bureau['BUREAU_CREDIT_ENDDATE_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n",
    "  \n",
    "    # 채무 금액 대비/대출 금액 비율 및 차이 가공\n",
    "    bureau['BUREAU_CREDIT_DEBT_RATIO']=bureau['AMT_CREDIT_SUM_DEBT']/bureau['AMT_CREDIT_SUM']\n",
    "    #bureau['BUREAU_CREDIT_DEBT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['BUREAU_CREDIT_DEBT_DIFF'] = bureau['AMT_CREDIT_SUM_DEBT'] - bureau['AMT_CREDIT_SUM']\n",
    "    \n",
    "    # 연체 여부 및 120일 이상 연체 여부 가공\n",
    "    bureau['BUREAU_IS_DPD'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    bureau['BUREAU_IS_DPD_OVER120'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x >120 else 0)\n",
    "    \n",
    "    return bureau\n",
    "\n",
    "# bureau 주요 컬럼 및 앞에서 채무 및 대출금액 관련 컬럼들로 SK_ID_CURR 레벨의 aggregation 컬럼 생성. \n",
    "def get_bureau_day_amt_agg(bureau):\n",
    "        \n",
    "    bureau_agg_dict = {\n",
    "    'SK_ID_BUREAU':['count'],\n",
    "    'DAYS_CREDIT':['min', 'max', 'mean'],\n",
    "    'CREDIT_DAY_OVERDUE':['min', 'max', 'mean'],\n",
    "    'DAYS_CREDIT_ENDDATE':['min', 'max', 'mean'],\n",
    "    'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
    "    'AMT_ANNUITY': ['max', 'mean', 'sum'],\n",
    "    # 추가 가공 컬럼\n",
    "    'BUREAU_ENDDATE_FACT_DIFF':['min', 'max', 'mean'],\n",
    "    'BUREAU_CREDIT_FACT_DIFF':['min', 'max', 'mean'],\n",
    "    'BUREAU_CREDIT_ENDDATE_DIFF':['min', 'max', 'mean'],\n",
    "    'BUREAU_CREDIT_DEBT_RATIO':['min', 'max', 'mean'],\n",
    "    'BUREAU_CREDIT_DEBT_DIFF':['min', 'max', 'mean'],\n",
    "    'BUREAU_IS_DPD':['mean', 'sum'],\n",
    "    'BUREAU_IS_DPD_OVER120':['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    bureau_grp = bureau.groupby('SK_ID_CURR')\n",
    "    bureau_day_amt_agg = bureau_grp.agg(bureau_agg_dict)\n",
    "    bureau_day_amt_agg.columns = ['BUREAU_'+('_').join(column).upper() for column in bureau_day_amt_agg.columns.ravel()]\n",
    "    # 조인을 위해 SK_ID_CURR을 reset_index()로 컬럼화 \n",
    "    bureau_day_amt_agg = bureau_day_amt_agg.reset_index()\n",
    "    #print('bureau_day_amt_agg shape:', bureau_day_amt_agg.shape)\n",
    "    return bureau_day_amt_agg\n",
    "\n",
    "# Bureau의 CREDIT_ACTIVE='Active' 인 데이터만 filtering 후 주요 컬럼 및 앞에서 채무 및 대출금액 관련 컬럼들로 SK_ID_CURR 레벨의 aggregation 컬럼 생성\n",
    "def get_bureau_active_agg(bureau):\n",
    "    # CREDIT_ACTIVE='Active' 인 데이터만 filtering\n",
    "    cond_active = bureau['CREDIT_ACTIVE'] == 'Active'\n",
    "    bureau_active_grp = bureau[cond_active].groupby(['SK_ID_CURR'])\n",
    "    bureau_agg_dict = {\n",
    "        'SK_ID_BUREAU':['count'],\n",
    "        'DAYS_CREDIT':['min', 'max', 'mean'],\n",
    "        'CREDIT_DAY_OVERDUE':['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_ENDDATE':['min', 'max', 'mean'],\n",
    "        'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean', 'sum'],\n",
    "        # 추가 가공 컬럼\n",
    "        'BUREAU_ENDDATE_FACT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_FACT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_ENDDATE_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_RATIO':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_IS_DPD':['mean', 'sum'],\n",
    "        'BUREAU_IS_DPD_OVER120':['mean', 'sum']\n",
    "        }\n",
    "\n",
    "    bureau_active_agg = bureau_active_grp.agg(bureau_agg_dict)\n",
    "    bureau_active_agg.columns = ['BUREAU_ACT_'+('_').join(column).upper() for column in bureau_active_agg.columns.ravel()]\n",
    "    # 조인을 위해 SK_ID_CURR을 reset_index()로 컬럼화 \n",
    "    bureau_active_agg = bureau_active_agg.reset_index()\n",
    "    #print('bureau_active_agg shape:', bureau_active_agg.shape)\n",
    "    return bureau_active_agg\n",
    "\n",
    "# BUREAU의 DAYS_CREDIT 가 -750보다 최근 데이터 별도 가공. \n",
    "def get_bureau_days750_agg(bureau):\n",
    "    cond_days750 = bureau['DAYS_CREDIT'] > -750\n",
    "    bureau_days750_group = bureau[cond_days750].groupby('SK_ID_CURR')\n",
    "    bureau_agg_dict = {\n",
    "        'SK_ID_BUREAU':['count'],\n",
    "        'DAYS_CREDIT':['min', 'max', 'mean'],\n",
    "        'CREDIT_DAY_OVERDUE':['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_ENDDATE':['min', 'max', 'mean'],\n",
    "        'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean', 'sum'],\n",
    "        # 추가 가공 컬럼\n",
    "        'BUREAU_ENDDATE_FACT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_FACT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_ENDDATE_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_RATIO':['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_DIFF':['min', 'max', 'mean'],\n",
    "        'BUREAU_IS_DPD':['mean', 'sum'],\n",
    "        'BUREAU_IS_DPD_OVER120':['mean', 'sum']\n",
    "        }\n",
    "\n",
    "    bureau_days750_agg = bureau_days750_group.agg(bureau_agg_dict)\n",
    "    bureau_days750_agg.columns = ['BUREAU_ACT_'+('_').join(column).upper() for column in bureau_days750_agg.columns.ravel()]\n",
    "    bureau_days750_agg = bureau_days750_agg.reset_index()\n",
    "    \n",
    "    return bureau_days750_agg\n",
    "\n",
    "\n",
    "# bureau_bal을 SK_ID_CURR 레벨로 건수와 MONTHS_BALANCE의 aggregation 가공 \n",
    "def get_bureau_bal_agg(bureau, bureau_bal):\n",
    "    # SK_ID_CURR레벨로 Group by하기 위해 bureau에서 SK_ID_CURR 컬럼을 가져오는 조인 수행. \n",
    "    bureau_bal = bureau_bal.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], on='SK_ID_BUREAU', how='left')\n",
    "    \n",
    "    # STATUS에 따라 월별 연체 여부 및 120일 이상 연체 여부 속성 가공. \n",
    "    bureau_bal['BUREAU_BAL_IS_DPD'] = bureau_bal['STATUS'].apply(lambda x: 1 if x in['1','2','3','4','5']  else 0)\n",
    "    bureau_bal['BUREAU_BAL_IS_DPD_OVER120'] = bureau_bal['STATUS'].apply(lambda x: 1 if x =='5'  else 0)\n",
    "    bureau_bal_grp = bureau_bal.groupby('SK_ID_CURR')\n",
    "    # SK_ID_CURR 레벨로 건수와 MONTHS_BALANCE의 aggregation 가공 \n",
    "    bureau_bal_agg_dict = {\n",
    "        'SK_ID_CURR':['count'],\n",
    "        'MONTHS_BALANCE':['min', 'max', 'mean'],\n",
    "        'BUREAU_BAL_IS_DPD':['mean', 'sum'],\n",
    "        'BUREAU_BAL_IS_DPD_OVER120':['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    bureau_bal_agg = bureau_bal_grp.agg(bureau_bal_agg_dict)\n",
    "    bureau_bal_agg.columns = [ 'BUREAU_BAL_'+('_').join(column).upper() for column in bureau_bal_agg.columns.ravel() ]\n",
    "    # 조인을 위해 SK_ID_CURR을 reset_index()로 컬럼화 \n",
    "    bureau_bal_agg = bureau_bal_agg.reset_index()\n",
    "    #print('bureau_bal_agg shape:', bureau_bal_agg.shape)\n",
    "    return bureau_bal_agg\n",
    "    \n",
    "# 가공된 bureau관련 aggregation 컬럼들을 모두 결합   \n",
    "def get_bureau_agg(bureau, bureau_bal):\n",
    "    \n",
    "    bureau = get_bureau_processed(bureau)\n",
    "    bureau_day_amt_agg = get_bureau_day_amt_agg(bureau)\n",
    "    bureau_active_agg = get_bureau_active_agg(bureau)\n",
    "    bureau_days750_agg = get_bureau_days750_agg(bureau)\n",
    "    bureau_bal_agg = get_bureau_bal_agg(bureau, bureau_bal)\n",
    "    \n",
    "    # bureau_day_amt_agg와 bureau_active_agg 조인.  \n",
    "    bureau_agg = bureau_day_amt_agg.merge(bureau_active_agg, on='SK_ID_CURR', how='left')\n",
    "    # STATUS가 ACTIVE IS_DPD RATIO관련 비율 재가공. \n",
    "    #bureau_agg['BUREAU_IS_DPD_RATIO'] = bureau_agg['BUREAU_BUREAU_IS_DPD_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n",
    "    #bureau_agg['BUREAU_IS_DPD_OVER120_RATIO'] = bureau_agg['BUREAU_BUREAU_IS_DPD_OVER120_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n",
    "    bureau_agg['BUREAU_ACT_IS_DPD_RATIO'] = bureau_agg['BUREAU_ACT_BUREAU_IS_DPD_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n",
    "    bureau_agg['BUREAU_ACT_IS_DPD_OVER120_RATIO'] = bureau_agg['BUREAU_ACT_BUREAU_IS_DPD_OVER120_SUM']/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n",
    "    \n",
    "    # bureau_agg와 bureau_bal_agg 조인. \n",
    "    bureau_agg = bureau_agg.merge(bureau_bal_agg, on='SK_ID_CURR', how='left')\n",
    "    bureau_agg = bureau_agg.merge(bureau_days750_agg, on='SK_ID_CURR', how='left') \n",
    "    #bureau_bal_agg['BUREAU_BAL_IS_DPD_RATIO'] = bureau_bal_agg['BUREAU_BAL_BUREAU_BAL_IS_DPD_SUM']/bureau_bal_agg['BUREAU_BAL_SK_ID_CURR_COUNT']\n",
    "    #bureau_bal_agg['BUREAU_BAL_IS_DPD_OVER120_RATIO'] = bureau_bal_agg['BUREAU_BAL_BUREAU_BAL_IS_DPD_OVER120_SUM']/bureau_bal_agg['BUREAU_BAL_SK_ID_CURR_COUNT']\n",
    "\n",
    "    #print('bureau_agg shape:', bureau_agg.shape)\n",
    "    \n",
    "    return bureau_agg\n",
    "\n",
    "def get_pos_bal_agg(pos_bal):\n",
    "  \n",
    "    # 연체여부,  연체일수 0~ 120 사이 여부, 연체 일수 120보다 큰 여부 \n",
    "    pos_bal['POS_IS_DPD'] = pos_bal['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    pos_bal['POS_IS_DPD_UNDER_120'] = pos_bal['SK_DPD'].apply(lambda x:1 if (x > 0) & (x <120) else 0 )\n",
    "    pos_bal['POS_IS_DPD_OVER_120'] = pos_bal['SK_DPD'].apply(lambda x:1 if x >= 120 else 0)\n",
    "\n",
    "    # 기존 컬럼과 신규 컬럼으로 SK_ID_CURR 레벨로 신규 aggregation 컬럼 생성\n",
    "    pos_bal_grp = pos_bal.groupby('SK_ID_CURR')\n",
    "    pos_bal_agg_dict = {\n",
    "        'SK_ID_CURR':['count'], \n",
    "        'MONTHS_BALANCE':['min', 'mean', 'max'], \n",
    "        'SK_DPD':['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT':['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT_FUTURE':['min', 'max', 'mean', 'sum'],\n",
    "        # 추가 컬럼. \n",
    "        'POS_IS_DPD':['mean', 'sum'],\n",
    "        'POS_IS_DPD_UNDER_120':['mean', 'sum'],\n",
    "        'POS_IS_DPD_OVER_120':['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    pos_bal_agg = pos_bal_grp.agg(pos_bal_agg_dict)\n",
    "    # 컬럼명 변경 \n",
    "    pos_bal_agg.columns = [('POS_')+('_').join(column).upper() for column in pos_bal_agg.columns.ravel()]\n",
    "    \n",
    "    # MONTHS_BALANCE가 최근(20개월 이하)인 데이터 세트 별도 가공. \n",
    "    cond_months = pos_bal['MONTHS_BALANCE'] > -20\n",
    "    pos_bal_m20_grp = pos_bal[cond_months].groupby('SK_ID_CURR')\n",
    "    pos_bal_m20_agg_dict = {\n",
    "        'SK_ID_CURR':['count'], \n",
    "        'MONTHS_BALANCE':['min', 'mean', 'max'], \n",
    "        'SK_DPD':['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT':['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT_FUTURE':['min', 'max', 'mean', 'sum'],\n",
    "        # 추가 컬럼. \n",
    "        'POS_IS_DPD':['mean', 'sum'],\n",
    "        'POS_IS_DPD_UNDER_120':['mean', 'sum'],\n",
    "        'POS_IS_DPD_OVER_120':['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    pos_bal_m20_agg = pos_bal_m20_grp.agg(pos_bal_m20_agg_dict)\n",
    "    # 컬럼 변경 \n",
    "    pos_bal_m20_agg.columns = [('POS_M20')+('_').join(column).upper() for column in pos_bal_m20_agg.columns.ravel()]\n",
    "    pos_bal_agg = pos_bal_agg.merge(pos_bal_m20_agg, on='SK_ID_CURR', how='left')\n",
    "    \n",
    "    # SK_ID_CURR을 reset_index()를 이용하여 컬럼으로 변환\n",
    "    pos_bal_agg = pos_bal_agg.reset_index()\n",
    "    \n",
    "    \n",
    "    return pos_bal_agg\n",
    "\n",
    "def get_install_agg(install):\n",
    "    # 예정 납부 금액 대비 실제 납부 금액 관련 데이터 가공. 예정 납부 일자 대비 실제 납부 일자 비교를 DPD 일자 생성  \n",
    "    install['AMT_DIFF'] = install['AMT_INSTALMENT'] - install['AMT_PAYMENT']\n",
    "    install['AMT_RATIO'] =  (install['AMT_PAYMENT'] +1)/ (install['AMT_INSTALMENT'] + 1)\n",
    "    install['SK_DPD'] = install['DAYS_ENTRY_PAYMENT'] - install['DAYS_INSTALMENT']\n",
    "\n",
    "    # 연체여부,  연체일수 30~ 120 사이 여부, 연체 일수 100보다 큰 여부 데이터 가공. \n",
    "    install['INS_IS_DPD'] = install['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    install['INS_IS_DPD_UNDER_120'] = install['SK_DPD'].apply(lambda x:1 if (x > 0) & (x <120) else 0 )\n",
    "    install['INS_IS_DPD_OVER_120'] = install['SK_DPD'].apply(lambda x:1 if x >= 120 else 0)\n",
    "\n",
    "    # 기존 컬럼과 신규 컬럼으로 SK_ID_CURR 레벨로 신규 aggregation 컬럼 생성. \n",
    "    install_grp = install.groupby('SK_ID_CURR')\n",
    "\n",
    "    install_agg_dict = {\n",
    "        'SK_ID_CURR':['count'],\n",
    "        'NUM_INSTALMENT_VERSION':['nunique'], \n",
    "        'DAYS_ENTRY_PAYMENT':['mean', 'max', 'sum'],\n",
    "        'DAYS_INSTALMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_INSTALMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_PAYMENT':['mean', 'max','sum'],\n",
    "        #  추가 컬럼\n",
    "        'AMT_DIFF':['mean','min', 'max','sum'],\n",
    "        'AMT_RATIO':['mean', 'max'],\n",
    "        'SK_DPD':['mean', 'min', 'max'],\n",
    "        'INS_IS_DPD':['mean', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120':['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120':['mean', 'sum']    \n",
    "    }\n",
    "\n",
    "    install_agg = install_grp.agg(install_agg_dict)\n",
    "    install_agg.columns = ['INS_'+('_').join(column).upper() for column in install_agg.columns.ravel()]\n",
    "\n",
    "    \n",
    "    # 실제 납부 일자(DAYS_ENTRY_PAYMENT)가 비교적 최근(1년 이내) 데이터만 별도로 가공\n",
    "    cond_day = install['DAYS_ENTRY_PAYMENT'] >= -365\n",
    "    install_d365_grp = install[cond_day].groupby('SK_ID_CURR')\n",
    "    install_d365_agg_dict = {\n",
    "        'SK_ID_CURR':['count'],\n",
    "        'NUM_INSTALMENT_VERSION':['nunique'], \n",
    "        'DAYS_ENTRY_PAYMENT':['mean', 'max', 'sum'],\n",
    "        'DAYS_INSTALMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_INSTALMENT':['mean', 'max', 'sum'],\n",
    "        'AMT_PAYMENT':['mean', 'max','sum'],\n",
    "        #  추가 컬럼\n",
    "        'AMT_DIFF':['mean','min', 'max','sum'],\n",
    "        'AMT_RATIO':['mean', 'max'],\n",
    "        'SK_DPD':['mean', 'min', 'max'],\n",
    "        'INS_IS_DPD':['mean', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120':['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120':['mean', 'sum']    \n",
    "    }\n",
    "    \n",
    "    install_d365_agg = install_d365_grp.agg(install_d365_agg_dict)\n",
    "    install_d365_agg.columns = ['INS_D365'+('_').join(column).upper() for column in install_d365_agg.columns.ravel()]\n",
    "    \n",
    "    install_agg = install_agg.merge(install_d365_agg, on='SK_ID_CURR', how='left')\n",
    "    install_agg = install_agg.reset_index()\n",
    "    \n",
    "    return install_agg\n",
    "\n",
    "def get_card_bal_agg(card_bal):\n",
    "    # 월별 카드 허용한도에 따른 잔고와 인출 금액 비율 \n",
    "    card_bal['BALANCE_LIMIT_RATIO'] = card_bal['AMT_BALANCE']/card_bal['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    card_bal['DRAWING_LIMIT_RATIO'] = card_bal['AMT_DRAWINGS_CURRENT'] / card_bal['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "\n",
    "    # DPD에 따른 가공 컬럼 생성.\n",
    "    card_bal['CARD_IS_DPD'] = card_bal['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    card_bal['CARD_IS_DPD_UNDER_120'] = card_bal['SK_DPD'].apply(lambda x:1 if (x > 0) & (x <120) else 0 )\n",
    "    card_bal['CARD_IS_DPD_OVER_120'] = card_bal['SK_DPD'].apply(lambda x:1 if x >= 120 else 0)\n",
    "\n",
    "    # 기존 컬럼과 가공 컬럼으로 SK_ID_CURR 레벨로 aggregation 신규 컬럼 생성. \n",
    "    card_bal_grp = card_bal.groupby('SK_ID_CURR')\n",
    "    card_bal_agg_dict = {\n",
    "        'SK_ID_CURR':['count'],\n",
    "         #'MONTHS_BALANCE':['min', 'max', 'mean'],\n",
    "        'AMT_BALANCE':['max'],\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL':['max'],\n",
    "        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n",
    "        'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n",
    "        'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n",
    "        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n",
    "        'AMT_PAYMENT_TOTAL_CURRENT': ['max','sum'],\n",
    "        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': ['max','sum'],\n",
    "        'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n",
    "        'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n",
    "        'SK_DPD': ['mean', 'max', 'sum'],\n",
    "        #  추가 컬럼\n",
    "        'BALANCE_LIMIT_RATIO':['min','max'],\n",
    "        'DRAWING_LIMIT_RATIO':['min', 'max'],\n",
    "        'CARD_IS_DPD':['mean', 'sum'],\n",
    "        'CARD_IS_DPD_UNDER_120':['mean', 'sum'],\n",
    "        'CARD_IS_DPD_OVER_120':['mean', 'sum']    \n",
    "    }\n",
    "    card_bal_agg = card_bal_grp.agg(card_bal_agg_dict)\n",
    "    card_bal_agg.columns = ['CARD_'+('_').join(column).upper() for column in card_bal_agg.columns.ravel()]\n",
    "\n",
    "    card_bal_agg = card_bal_agg.reset_index()\n",
    "    \n",
    "    # MONTHS_BALANCE가 비교적 최근 데이터( 3개월 이하)만 별도로 가공.  \n",
    "    cond_month = card_bal.MONTHS_BALANCE >= -3\n",
    "    card_bal_m3_grp = card_bal[cond_month].groupby('SK_ID_CURR')\n",
    "    card_bal_m3_agg = card_bal_m3_grp.agg(card_bal_agg_dict)\n",
    "    card_bal_m3_agg.columns = ['CARD_M3'+('_').join(column).upper() for column in card_bal_m3_agg.columns.ravel()]\n",
    "    \n",
    "    card_bal_agg = card_bal_agg.merge(card_bal_m3_agg, on='SK_ID_CURR', how='left')\n",
    "    card_bal_agg = card_bal_agg.reset_index()\n",
    "    \n",
    "    return card_bal_agg\n",
    "\n",
    "def get_apps_all_encoded(apps_all):\n",
    "    object_columns = apps_all.dtypes[apps_all.dtypes == 'object'].index.tolist()\n",
    "    for column in object_columns:\n",
    "        apps_all[column] = pd.factorize(apps_all[column])[0]\n",
    "    \n",
    "    return apps_all\n",
    "\n",
    "def get_apps_all_train_test(apps_all):\n",
    "    apps_all_train = apps_all[~apps_all['TARGET'].isnull()]\n",
    "    apps_all_test = apps_all[apps_all['TARGET'].isnull()]\n",
    "\n",
    "    apps_all_test = apps_all_test.drop('TARGET', axis=1)\n",
    "    \n",
    "    return apps_all_train, apps_all_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "plastic-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apps와 prev_agg, bureau_agg, pos_bal_agg, install_agg, card_bal_agg를 개별 함수 호출하여 생성후 조인 결합\n",
    "def get_apps_all_with_all_agg(apps, prev, bureau, bureau_bal, pos_bal, install, card_bal):\n",
    "    apps_all =  get_apps_processed(apps)\n",
    "    prev_agg = get_prev_agg(prev)\n",
    "    bureau_agg = get_bureau_agg(bureau, bureau_bal)\n",
    "    pos_bal_agg = get_pos_bal_agg(pos_bal)\n",
    "    install_agg = get_install_agg(install)\n",
    "    card_bal_agg = get_card_bal_agg(card_bal)\n",
    "    print('prev_agg shape:', prev_agg.shape, 'bureau_agg shape:', bureau_agg.shape )\n",
    "    print('pos_bal_agg shape:', pos_bal_agg.shape, 'install_agg shape:', install_agg.shape, 'card_bal_agg shape:', card_bal_agg.shape)\n",
    "    print('apps_all before merge shape:', apps_all.shape)\n",
    "    \n",
    "    # 생성된 prev_agg, bureau_agg, pos_bal_agg, install_agg, card_bal_agg를 apps와 조인하여 최종 학습/테스트 집합 생성. \n",
    "    apps_all = apps_all.merge(prev_agg, on='SK_ID_CURR', how='left')\n",
    "    apps_all = apps_all.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "    apps_all = apps_all.merge(pos_bal_agg, on='SK_ID_CURR', how='left')\n",
    "    apps_all = apps_all.merge(install_agg, on='SK_ID_CURR', how='left')\n",
    "    apps_all = apps_all.merge(card_bal_agg, on='SK_ID_CURR', how='left')\n",
    "    \n",
    "    print('apps_all after merge with all shape:', apps_all.shape)\n",
    "    \n",
    "    return apps_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-constraint",
   "metadata": {},
   "source": [
    "### Final training/testing set based on feature processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polyphonic-lewis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-a642f2f91390>:79: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  prev_amt_agg.columns = [\"PREV_\"+ \"_\".join(x).upper() for x in prev_amt_agg.columns.ravel()]\n",
      "<ipython-input-3-a642f2f91390>:121: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  prev_days365_agg.columns = [\"PREV_D365_\"+ \"_\".join(x).upper() for x in prev_days365_agg.columns.ravel()]\n",
      "<ipython-input-3-a642f2f91390>:186: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  bureau_day_amt_agg.columns = ['BUREAU_'+('_').join(column).upper() for column in bureau_day_amt_agg.columns.ravel()]\n",
      "<ipython-input-3-a642f2f91390>:219: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  bureau_active_agg.columns = ['BUREAU_ACT_'+('_').join(column).upper() for column in bureau_active_agg.columns.ravel()]\n",
      "<ipython-input-3-a642f2f91390>:251: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  bureau_days750_agg.columns = ['BUREAU_ACT_'+('_').join(column).upper() for column in bureau_days750_agg.columns.ravel()]\n",
      "<ipython-input-3-a642f2f91390>:275: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  bureau_bal_agg.columns = [ 'BUREAU_BAL_'+('_').join(column).upper() for column in bureau_bal_agg.columns.ravel() ]\n",
      "<ipython-input-3-a642f2f91390>:331: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  pos_bal_agg.columns = [('POS_')+('_').join(column).upper() for column in pos_bal_agg.columns.ravel()]\n",
      "<ipython-input-3-a642f2f91390>:350: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  pos_bal_m20_agg.columns = [('POS_M20')+('_').join(column).upper() for column in pos_bal_m20_agg.columns.ravel()]\n",
      "<ipython-input-3-a642f2f91390>:390: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  install_agg.columns = ['INS_'+('_').join(column).upper() for column in install_agg.columns.ravel()]\n",
      "<ipython-input-3-a642f2f91390>:413: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  install_d365_agg.columns = ['INS_D365'+('_').join(column).upper() for column in install_d365_agg.columns.ravel()]\n",
      "<ipython-input-3-a642f2f91390>:455: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  card_bal_agg.columns = ['CARD_'+('_').join(column).upper() for column in card_bal_agg.columns.ravel()]\n",
      "<ipython-input-3-a642f2f91390>:463: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  card_bal_m3_agg.columns = ['CARD_M3'+('_').join(column).upper() for column in card_bal_m3_agg.columns.ravel()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev_agg shape: (338857, 80) bureau_agg shape: (305811, 149)\n",
      "pos_bal_agg shape: (337252, 45) install_agg shape: (339587, 59) card_bal_agg shape: (103558, 70)\n",
      "apps_all before merge shape: (356255, 135)\n",
      "apps_all after merge with all shape: (356255, 534)\n"
     ]
    }
   ],
   "source": [
    "apps, prev, bureau, bureau_bal, pos_bal, install, card_bal = get_dataset()\n",
    "\n",
    "apps_all = get_apps_all_with_all_agg(apps, prev, bureau, bureau_bal, pos_bal, install, card_bal)\n",
    "\n",
    "apps_all = get_apps_all_encoded(apps_all)\n",
    "\n",
    "apps_all_train, apps_all_test = get_apps_all_train_test(apps_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-bradley",
   "metadata": {},
   "source": [
    "### Out Of Fold Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "emotional-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_apps_all_with_oof(apps_all_train, apps_all_test, nfolds=5):\n",
    "    ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "    target_app = apps_all_train['TARGET']\n",
    "    \n",
    "    # KFold generation \n",
    "    folds = KFold(n_splits=nfolds, shuffle=True, random_state=2020)\n",
    "    \n",
    "    # prediction result saving location for selected validation set\n",
    "    # total size = validation set size * nsplit = ftr_app size\n",
    "    oof_preds = np.zeros(ftr_app.shape[0])\n",
    "    # prediction result saving location for testing dataset\n",
    "    test_preds = np.zeros(apps_all_test.shape[0])\n",
    "    \n",
    "    clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=4000,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=11,\n",
    "            num_leaves=58,\n",
    "            colsample_bytree=0.613,\n",
    "            subsample=0.708,\n",
    "            max_bin=407,\n",
    "            reg_alpha=3.564,\n",
    "            reg_lambda=4.930,\n",
    "            min_child_weight=6,\n",
    "            min_child_samples=165,\n",
    "            silent=-1,\n",
    "            verbose=-1\n",
    "    )\n",
    "    # iteration for nfolds times, and apply out of fold (OOF)\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds.split(ftr_app)):\n",
    "        print('##### iteration ', fold_idx, 'start')\n",
    "        train_x = ftr_app.iloc[train_idx,:]\n",
    "        train_y = target_app.iloc[train_idx]\n",
    "        valid_x = ftr_app.iloc[valid_idx,:]\n",
    "        valid_y = target_app.iloc[valid_idx]\n",
    "        \n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric='auc', verbose=200, early_stopping_rounds=200)\n",
    "        # prediction results using validation set\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        # pridiction results using testing set and averaging\n",
    "        test_preds += clf.predict_proba(apps_all_test.drop('SK_ID_CURR', axis=1), num_iteration=clf.best_iteration_)[:, 1]/folds.n_splits\n",
    "    \n",
    "    return clf, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "numeric-secret",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-18 12:50:48.175873\n",
      "##### iteration  0 start\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.795116\ttraining's binary_logloss: 0.239641\tvalid_1's auc: 0.772003\tvalid_1's binary_logloss: 0.24236\n",
      "[400]\ttraining's auc: 0.817968\ttraining's binary_logloss: 0.228438\tvalid_1's auc: 0.783305\tvalid_1's binary_logloss: 0.236454\n",
      "[600]\ttraining's auc: 0.833738\ttraining's binary_logloss: 0.221271\tvalid_1's auc: 0.788382\tvalid_1's binary_logloss: 0.234164\n",
      "[800]\ttraining's auc: 0.847061\ttraining's binary_logloss: 0.215461\tvalid_1's auc: 0.791171\tvalid_1's binary_logloss: 0.232963\n",
      "[1000]\ttraining's auc: 0.858636\ttraining's binary_logloss: 0.210456\tvalid_1's auc: 0.792925\tvalid_1's binary_logloss: 0.232243\n",
      "[1200]\ttraining's auc: 0.868979\ttraining's binary_logloss: 0.205926\tvalid_1's auc: 0.794007\tvalid_1's binary_logloss: 0.2318\n",
      "[1400]\ttraining's auc: 0.8784\ttraining's binary_logloss: 0.20172\tvalid_1's auc: 0.794539\tvalid_1's binary_logloss: 0.23155\n",
      "[1600]\ttraining's auc: 0.886955\ttraining's binary_logloss: 0.197778\tvalid_1's auc: 0.795025\tvalid_1's binary_logloss: 0.231349\n",
      "[1800]\ttraining's auc: 0.894612\ttraining's binary_logloss: 0.194085\tvalid_1's auc: 0.795268\tvalid_1's binary_logloss: 0.231226\n",
      "[2000]\ttraining's auc: 0.901705\ttraining's binary_logloss: 0.190567\tvalid_1's auc: 0.795416\tvalid_1's binary_logloss: 0.231148\n",
      "[2200]\ttraining's auc: 0.908341\ttraining's binary_logloss: 0.18717\tvalid_1's auc: 0.795571\tvalid_1's binary_logloss: 0.231094\n",
      "[2400]\ttraining's auc: 0.914643\ttraining's binary_logloss: 0.183876\tvalid_1's auc: 0.795708\tvalid_1's binary_logloss: 0.23104\n",
      "[2600]\ttraining's auc: 0.920414\ttraining's binary_logloss: 0.18074\tvalid_1's auc: 0.795804\tvalid_1's binary_logloss: 0.231011\n",
      "[2800]\ttraining's auc: 0.925784\ttraining's binary_logloss: 0.177682\tvalid_1's auc: 0.795805\tvalid_1's binary_logloss: 0.230996\n",
      "[3000]\ttraining's auc: 0.93073\ttraining's binary_logloss: 0.174736\tvalid_1's auc: 0.795961\tvalid_1's binary_logloss: 0.230949\n",
      "Early stopping, best iteration is:\n",
      "[2993]\ttraining's auc: 0.930559\ttraining's binary_logloss: 0.174834\tvalid_1's auc: 0.795965\tvalid_1's binary_logloss: 0.23095\n",
      "##### iteration  1 start\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.79552\ttraining's binary_logloss: 0.238223\tvalid_1's auc: 0.769661\tvalid_1's binary_logloss: 0.247772\n",
      "[400]\ttraining's auc: 0.818281\ttraining's binary_logloss: 0.227069\tvalid_1's auc: 0.782162\tvalid_1's binary_logloss: 0.241681\n",
      "[600]\ttraining's auc: 0.834118\ttraining's binary_logloss: 0.219906\tvalid_1's auc: 0.788259\tvalid_1's binary_logloss: 0.239197\n",
      "[800]\ttraining's auc: 0.847239\ttraining's binary_logloss: 0.214165\tvalid_1's auc: 0.79137\tvalid_1's binary_logloss: 0.237994\n",
      "[1000]\ttraining's auc: 0.858686\ttraining's binary_logloss: 0.209172\tvalid_1's auc: 0.793087\tvalid_1's binary_logloss: 0.237331\n",
      "[1200]\ttraining's auc: 0.868959\ttraining's binary_logloss: 0.20463\tvalid_1's auc: 0.794161\tvalid_1's binary_logloss: 0.236923\n",
      "[1400]\ttraining's auc: 0.87808\ttraining's binary_logloss: 0.200487\tvalid_1's auc: 0.794868\tvalid_1's binary_logloss: 0.236645\n",
      "[1600]\ttraining's auc: 0.886516\ttraining's binary_logloss: 0.196591\tvalid_1's auc: 0.795198\tvalid_1's binary_logloss: 0.236517\n",
      "[1800]\ttraining's auc: 0.894268\ttraining's binary_logloss: 0.192917\tvalid_1's auc: 0.795618\tvalid_1's binary_logloss: 0.236373\n",
      "[2000]\ttraining's auc: 0.901454\ttraining's binary_logloss: 0.189436\tvalid_1's auc: 0.795747\tvalid_1's binary_logloss: 0.236331\n",
      "[2200]\ttraining's auc: 0.90802\ttraining's binary_logloss: 0.186051\tvalid_1's auc: 0.795948\tvalid_1's binary_logloss: 0.236258\n",
      "[2400]\ttraining's auc: 0.91425\ttraining's binary_logloss: 0.182805\tvalid_1's auc: 0.796037\tvalid_1's binary_logloss: 0.236253\n",
      "[2600]\ttraining's auc: 0.92004\ttraining's binary_logloss: 0.17965\tvalid_1's auc: 0.796219\tvalid_1's binary_logloss: 0.236209\n",
      "[2800]\ttraining's auc: 0.925392\ttraining's binary_logloss: 0.176599\tvalid_1's auc: 0.796303\tvalid_1's binary_logloss: 0.236207\n",
      "Early stopping, best iteration is:\n",
      "[2699]\ttraining's auc: 0.922768\ttraining's binary_logloss: 0.178105\tvalid_1's auc: 0.796314\tvalid_1's binary_logloss: 0.236183\n",
      "##### iteration  2 start\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.795876\ttraining's binary_logloss: 0.238765\tvalid_1's auc: 0.767197\tvalid_1's binary_logloss: 0.245633\n",
      "[400]\ttraining's auc: 0.8185\ttraining's binary_logloss: 0.227573\tvalid_1's auc: 0.78009\tvalid_1's binary_logloss: 0.239628\n",
      "[600]\ttraining's auc: 0.834289\ttraining's binary_logloss: 0.220363\tvalid_1's auc: 0.786225\tvalid_1's binary_logloss: 0.237225\n",
      "[800]\ttraining's auc: 0.847188\ttraining's binary_logloss: 0.214616\tvalid_1's auc: 0.789549\tvalid_1's binary_logloss: 0.235986\n",
      "[1000]\ttraining's auc: 0.858617\ttraining's binary_logloss: 0.209595\tvalid_1's auc: 0.791237\tvalid_1's binary_logloss: 0.235332\n",
      "[1200]\ttraining's auc: 0.868884\ttraining's binary_logloss: 0.205071\tvalid_1's auc: 0.792256\tvalid_1's binary_logloss: 0.23495\n",
      "[1400]\ttraining's auc: 0.878429\ttraining's binary_logloss: 0.200863\tvalid_1's auc: 0.792793\tvalid_1's binary_logloss: 0.234736\n",
      "[1600]\ttraining's auc: 0.886918\ttraining's binary_logloss: 0.196988\tvalid_1's auc: 0.793215\tvalid_1's binary_logloss: 0.234571\n",
      "[1800]\ttraining's auc: 0.894942\ttraining's binary_logloss: 0.193261\tvalid_1's auc: 0.793514\tvalid_1's binary_logloss: 0.234452\n",
      "[2000]\ttraining's auc: 0.902154\ttraining's binary_logloss: 0.189708\tvalid_1's auc: 0.793766\tvalid_1's binary_logloss: 0.234367\n",
      "[2200]\ttraining's auc: 0.908779\ttraining's binary_logloss: 0.186328\tvalid_1's auc: 0.793973\tvalid_1's binary_logloss: 0.234289\n",
      "[2400]\ttraining's auc: 0.914882\ttraining's binary_logloss: 0.183075\tvalid_1's auc: 0.79404\tvalid_1's binary_logloss: 0.234269\n",
      "[2600]\ttraining's auc: 0.920759\ttraining's binary_logloss: 0.179883\tvalid_1's auc: 0.794272\tvalid_1's binary_logloss: 0.234188\n",
      "[2800]\ttraining's auc: 0.926457\ttraining's binary_logloss: 0.176731\tvalid_1's auc: 0.794404\tvalid_1's binary_logloss: 0.234154\n",
      "Early stopping, best iteration is:\n",
      "[2793]\ttraining's auc: 0.92629\ttraining's binary_logloss: 0.176833\tvalid_1's auc: 0.794413\tvalid_1's binary_logloss: 0.234149\n",
      "##### iteration  3 start\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.796117\ttraining's binary_logloss: 0.239076\tvalid_1's auc: 0.767778\tvalid_1's binary_logloss: 0.244097\n",
      "[400]\ttraining's auc: 0.818822\ttraining's binary_logloss: 0.227825\tvalid_1's auc: 0.779405\tvalid_1's binary_logloss: 0.238376\n",
      "[600]\ttraining's auc: 0.834561\ttraining's binary_logloss: 0.220597\tvalid_1's auc: 0.785305\tvalid_1's binary_logloss: 0.236103\n",
      "[800]\ttraining's auc: 0.847251\ttraining's binary_logloss: 0.214933\tvalid_1's auc: 0.788418\tvalid_1's binary_logloss: 0.23497\n",
      "[1000]\ttraining's auc: 0.858574\ttraining's binary_logloss: 0.209915\tvalid_1's auc: 0.790394\tvalid_1's binary_logloss: 0.234311\n",
      "[1200]\ttraining's auc: 0.869064\ttraining's binary_logloss: 0.205308\tvalid_1's auc: 0.791724\tvalid_1's binary_logloss: 0.233899\n",
      "[1400]\ttraining's auc: 0.878295\ttraining's binary_logloss: 0.201131\tvalid_1's auc: 0.792512\tvalid_1's binary_logloss: 0.233663\n",
      "[1600]\ttraining's auc: 0.886652\ttraining's binary_logloss: 0.197253\tvalid_1's auc: 0.792892\tvalid_1's binary_logloss: 0.233534\n",
      "[1800]\ttraining's auc: 0.89446\ttraining's binary_logloss: 0.193594\tvalid_1's auc: 0.792994\tvalid_1's binary_logloss: 0.233498\n",
      "[2000]\ttraining's auc: 0.901693\ttraining's binary_logloss: 0.190098\tvalid_1's auc: 0.793098\tvalid_1's binary_logloss: 0.233465\n",
      "[2200]\ttraining's auc: 0.908432\ttraining's binary_logloss: 0.186694\tvalid_1's auc: 0.793174\tvalid_1's binary_logloss: 0.233446\n",
      "[2400]\ttraining's auc: 0.914683\ttraining's binary_logloss: 0.183395\tvalid_1's auc: 0.793293\tvalid_1's binary_logloss: 0.233424\n",
      "Early stopping, best iteration is:\n",
      "[2384]\ttraining's auc: 0.914176\ttraining's binary_logloss: 0.18366\tvalid_1's auc: 0.793301\tvalid_1's binary_logloss: 0.233419\n",
      "##### iteration  4 start\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.794309\ttraining's binary_logloss: 0.238569\tvalid_1's auc: 0.776034\tvalid_1's binary_logloss: 0.246881\n",
      "[400]\ttraining's auc: 0.817214\ttraining's binary_logloss: 0.227384\tvalid_1's auc: 0.788029\tvalid_1's binary_logloss: 0.240505\n",
      "[600]\ttraining's auc: 0.832928\ttraining's binary_logloss: 0.220258\tvalid_1's auc: 0.793487\tvalid_1's binary_logloss: 0.238024\n",
      "[800]\ttraining's auc: 0.84601\ttraining's binary_logloss: 0.214531\tvalid_1's auc: 0.79657\tvalid_1's binary_logloss: 0.236762\n",
      "[1000]\ttraining's auc: 0.857802\ttraining's binary_logloss: 0.209492\tvalid_1's auc: 0.79861\tvalid_1's binary_logloss: 0.235983\n",
      "[1200]\ttraining's auc: 0.867894\ttraining's binary_logloss: 0.205\tvalid_1's auc: 0.799901\tvalid_1's binary_logloss: 0.235496\n",
      "[1400]\ttraining's auc: 0.87711\ttraining's binary_logloss: 0.200876\tvalid_1's auc: 0.800669\tvalid_1's binary_logloss: 0.235198\n",
      "[1600]\ttraining's auc: 0.885719\ttraining's binary_logloss: 0.196965\tvalid_1's auc: 0.801237\tvalid_1's binary_logloss: 0.234983\n",
      "[1800]\ttraining's auc: 0.893544\ttraining's binary_logloss: 0.193275\tvalid_1's auc: 0.801688\tvalid_1's binary_logloss: 0.234813\n",
      "[2000]\ttraining's auc: 0.900876\ttraining's binary_logloss: 0.189739\tvalid_1's auc: 0.801858\tvalid_1's binary_logloss: 0.234748\n",
      "[2200]\ttraining's auc: 0.907566\ttraining's binary_logloss: 0.186347\tvalid_1's auc: 0.802129\tvalid_1's binary_logloss: 0.234656\n",
      "[2400]\ttraining's auc: 0.914035\ttraining's binary_logloss: 0.183071\tvalid_1's auc: 0.802082\tvalid_1's binary_logloss: 0.234682\n",
      "Early stopping, best iteration is:\n",
      "[2254]\ttraining's auc: 0.909418\ttraining's binary_logloss: 0.185439\tvalid_1's auc: 0.802177\tvalid_1's binary_logloss: 0.234649\n",
      "2021-04-18 13:44:59.702641\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "clf, test_preds = train_apps_all_with_oof(apps_all_train, apps_all_test, nfolds=5)\n",
    "apps_all_test['TARGET'] = test_preds\n",
    "apps_all_test[['SK_ID_CURR', 'TARGET']].to_csv('pred_results_with_oof.csv', index=False)\n",
    "\n",
    "print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-extra",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
